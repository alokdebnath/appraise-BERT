{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f377a5f5-76e1-4f77-9857-e6e7afa3aa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Validation function\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, labels in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total_samples\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# Training loop with mixed precision and gradient checkpointing\n",
    "def train_model(model, dataloader, optimizer, scheduler, criterion, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Model and tokenizer setup\n",
    "    tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=NUM_EMOTIONS)\n",
    "    model.gradient_checkpointing_enable()  # Enable gradient checkpointing\n",
    "    \n",
    "    # Mixed precision training setup\n",
    "    scaler = GradScaler()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72327c3-222f-460c-96c9-f7e59cf9fc4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Define the Appraisal MLP model\n",
    "class AppraisalMLP(nn.Module):\n",
    "    def __init__(self, input_dim, num_emotions):\n",
    "        super(AppraisalMLP, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Training loop with mixed precision and gradient checkpointing\n",
    "def train_appraisal_model(model, dataloader, optimizer, criterion, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for appraisal_inputs, labels in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            appraisal_inputs = appraisal_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            with autocast():  # Mixed precision context\n",
    "                outputs = model(appraisal_inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                total_train_loss += loss.item()\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(dataloader)\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "# Example of creating the optimizer and criterion\n",
    "input_dim = 10  # Change to match your dataset's number of appraisal dimensions\n",
    "num_emotions = 5  # Change to match the number of emotion classes\n",
    "model = AppraisalMLP(input_dim=input_dim, num_emotions=num_emotions)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Assume `train_dataloader` and `val_dataloader` are already defined\n",
    "train_appraisal_model(model, train_dataloader, optimizer, criterion, device='cuda', num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a436618-71c3-41c8-b656-8392816b8b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, num_emotions, appraisal_input_dim):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.text_model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=num_emotions)\n",
    "        self.text_model.gradient_checkpointing_enable()  # Enable gradient checkpointing for the text model\n",
    "        self.appraisal_model = AppraisalMLP(input_dim=appraisal_input_dim, num_emotions=num_emotions)\n",
    "        self.fc_combined = nn.Sequential(\n",
    "            nn.Linear(num_emotions * 2, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_emotions)\n",
    "        )\n",
    "    \n",
    "    def forward(self, text_inputs, appraisal_inputs):\n",
    "        text_outputs = self.text_model(**text_inputs).logits\n",
    "        appraisal_outputs = self.appraisal_model(appraisal_inputs)\n",
    "        combined = torch.cat((text_outputs, appraisal_outputs), dim=1)\n",
    "        return self.fc_combined(combined)\n",
    "\n",
    "# Combined training with mixed precision\n",
    "def train_combined_model(model, text_dataloader, appraisal_dataloader, optimizer, scheduler, criterion, device, num_epochs=5):\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for (text_batch, text_labels), appraisal_inputs in zip(text_dataloader, appraisal_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            text_batch = {k: v.to(device) for k, v in text_batch.items()}\n",
    "            text_labels = text_labels.to(device)\n",
    "            appraisal_inputs = appraisal_inputs.to(device)\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(text_batch, appraisal_inputs)\n",
    "                loss = criterion(outputs, text_labels)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "        val_loss, val_accuracy = evaluate(model, val_dataloader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d187bcbb-b45f-4e46-a795-e44f048e430a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch, labels in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(**batch)\n",
    "                loss = criterion(outputs.logits, labels)\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            preds = torch.argmax(outputs.logits, dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total_samples\n",
    "    return avg_loss, accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
